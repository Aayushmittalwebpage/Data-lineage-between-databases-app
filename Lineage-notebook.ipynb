{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking data lineage between two isolated databases\n",
    "\n",
    "Author: Aayush Mittal,\n",
    "Last edited: 08-07-2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all libraries\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "import scipy.stats\n",
    "from scipy.stats import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import statsmodels.api as sm \n",
    "import seaborn as sns\n",
    "import pylab as py \n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "import networkx as nx\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Extracting Data from database and setting up in python environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputting and storing variables of Database 1\n",
    "\n",
    "database_name1 = input(\"Enter database name >>>\")\n",
    "host1 = input(\"Enter host >>>\")\n",
    "password1 = input(\"Enter password >>>\")\n",
    "port1 = input(\"Enter port >>>\")\n",
    "user_name1 = input(\"Enter username >>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputting and storing variables of Database 2\n",
    "\n",
    "database_name2 = input(\"Enter database name >>>\")\n",
    "host2 = input(\"Enter host >>>\")\n",
    "password2 = input(\"Enter password >>>\")\n",
    "port2 = input(\"Enter port >>>\")\n",
    "user_name2 = input(\"Enter username >>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sql-alchemy to input database variables and set up the database in python environment\n",
    "#input database must be postgresql databse\n",
    "\n",
    "engine1 = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    user_name1, \n",
    "    password1, \n",
    "    host1, \n",
    "    port1, \n",
    "    database_name1 ))\n",
    "\n",
    "\n",
    "\n",
    "engine2 = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    user_name2, \n",
    "    password2, \n",
    "    host2, \n",
    "    port2, \n",
    "    database_name2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name of pandas dataframe will be \"database-name\"__\"table-name\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through all the tables and store in different pandas dataframe\n",
    "\n",
    "list_table1 = engine1.table_names()\n",
    "\n",
    "for i in range(len(list_table1)):\n",
    "    globals()['{}__{}'.format(database_name1, list_table1[i]) ] = pd.read_sql_table(list_table1[i], engine1)\n",
    "    \n",
    "    \n",
    "    \n",
    "list_table2 = engine2.table_names()\n",
    "\n",
    "for i in range(len(list_table2)):\n",
    "    globals()['{}__{}'.format(database_name2, list_table2[i]) ] = pd.read_sql_table(list_table2[i], engine2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self defined Functions used in the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION THAT FINDS THE BEST FIT DISTRIBUTION OF A ATTRIBUTE\n",
    "\n",
    "# standarise the column of a particular dataset, which is used in finding data distribution\n",
    "def standarise(df, column,pct,pct_lower):\n",
    "    sc = StandardScaler() \n",
    "    y = df[column][df[column].notnull()].to_list()\n",
    "    y.sort()\n",
    "    len_y = len(y)\n",
    "    y = y[int(pct_lower * len_y):int(len_y * pct)]\n",
    "    len_y = len(y)\n",
    "    yy=([[x] for x in y])\n",
    "    sc.fit(yy)\n",
    "    y_std =sc.transform(yy)\n",
    "    y_std = y_std.flatten()\n",
    "    return y_std,len_y,y\n",
    "\n",
    "# returns which distribution best fits the data attribute\n",
    "def fit_distribution(df, column,pct,pct_lower):\n",
    "    # Set up list of candidate distributions to use\n",
    "\n",
    "    y_std,size,y_org = standarise(df,column,pct,pct_lower)\n",
    "    dist_names = ['weibull_min','norm','weibull_max','beta',\n",
    "                 'invgauss','uniform','gamma','expon', 'pearson3','triang']\n",
    "\n",
    "    chi_square_statistics = []\n",
    "    # 11 bins\n",
    "    percentile_bins = np.linspace(0,100,11)\n",
    "    percentile_cutoffs = np.percentile(y_std, percentile_bins)\n",
    "    observed_frequency, bins = (np.histogram(y_std, bins=percentile_cutoffs))\n",
    "    cum_observed_frequency = np.cumsum(observed_frequency)\n",
    "\n",
    "    # Loop through candidate distributions\n",
    "\n",
    "    for distribution in dist_names:\n",
    "        # Set up distribution and get fitted distribution parameters\n",
    "        dist = getattr(scipy.stats, distribution)\n",
    "        param = dist.fit(y_std)\n",
    "      \n",
    "\n",
    "        # Get expected counts in percentile bins\n",
    "        # cdf of fitted sistrinution across bins\n",
    "        cdf_fitted = dist.cdf(percentile_cutoffs, *param)\n",
    "        expected_frequency = []\n",
    "        for bin in range(len(percentile_bins)-1):\n",
    "            expected_cdf_area = cdf_fitted[bin+1] - cdf_fitted[bin]\n",
    "            expected_frequency.append(expected_cdf_area)\n",
    "\n",
    "        # Chi-square Statistics\n",
    "        expected_frequency = np.array(expected_frequency) * size\n",
    "        cum_expected_frequency = np.cumsum(expected_frequency)\n",
    "        ss = round(sum (((cum_expected_frequency - cum_observed_frequency) ** 2) / cum_observed_frequency),0)\n",
    "        chi_square_statistics.append(ss)\n",
    "\n",
    "\n",
    "    #Sort by minimum ch-square statistics\n",
    "    results = pd.DataFrame()\n",
    "    results['Distribution'] = dist_names\n",
    "    results['chi_square'] = chi_square_statistics\n",
    "    results.sort_values(['chi_square'], inplace=True)\n",
    "    \n",
    "    #returns distribution with lowest chi sqaure value or highest fitness\n",
    "    return results.iloc[0,0]\n",
    "\n",
    "\n",
    "# Function that calculates Jaccard similarity between 2 sets of data\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "\n",
    "# Function that extracts name of dataset\n",
    "\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date attributes are converted into numerical value by finding number of days b/w today and the date in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all dates attributes into numerical value\n",
    "    \n",
    "def preprocess_date(df):\n",
    "    import datetime\n",
    "    df['today'] = datetime.date.today()\n",
    "    \n",
    "    filteredColumns_date = df.dtypes[df.dtypes == np.dtype('<M8[ns]') ]\n",
    "    # list of columns whose data type is date\n",
    "    listOfColumnNames_date = list(filteredColumns_date.index)\n",
    "    \n",
    "    for p in listOfColumnNames_date:\n",
    "        \n",
    "        df[p+'_num_days'] = (pd.to_datetime(df['today']) - pd.to_datetime(df[p])).dt.days\n",
    "        df.drop([p], axis = 1, inplace=True)\n",
    "\n",
    "    df.drop(['today'], axis = 1, inplace=True)   \n",
    "    \n",
    "\n",
    "# passing all tables from both datasets in the preprocess_date function \n",
    "for i in range(len(list_table1)):\n",
    "    preprocess_date(globals()['{}__{}'.format(database_name1, list_table1[i]) ])\n",
    "    \n",
    "    \n",
    "for i in range(len(list_table2)):\n",
    "    preprocess_date(globals()['{}__{}'.format(database_name2, list_table2[i]) ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these 2 list will be filled throughout the compare_two_table function and will be used in the end to create the lineage graph visualisation\n",
    "list_lineage_source=[]\n",
    "list_lineage_target=[]\n",
    "\n",
    "#these 3 list will be filled throughout the compare_two_table function and will be used in the end to create the Tabular lineage report\n",
    "\n",
    "attribute1=[]\n",
    "attribute2=[]\n",
    "metric=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates Comparator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all date attributes b/w the 2 datasets and finds similarity and replication using Jaccard similarity, distribution test and other statistics\n",
    "def dates_compare(df1, df2, list_num):\n",
    "    \n",
    "    #initialising overall similarity and replication counter\n",
    "    s1=0\n",
    "    r1=0\n",
    "\n",
    "    for p in list_num:\n",
    "        \n",
    "        #only columns ending with _num_days (dates column) will be processed\n",
    "        x = re.search('_num_days$', p[0])\n",
    "        y = re.search('_num_days$', p[1])\n",
    "\n",
    "        #only if both columns ends with _num_days (dates column) then will be processed\n",
    "        if(x and y):\n",
    "\n",
    "\n",
    "            len1 = len(df1[p[0]])\n",
    "            len2 = len(df2[p[1]])\n",
    "\n",
    "            numpy_df1 = df1[p[0]].dropna()\n",
    "            numpy_df2 = df2[p[1]].dropna()\n",
    "\n",
    "            numpy_arr1=numpy_df1.to_numpy()\n",
    "            numpy_arr2=numpy_df2.to_numpy()\n",
    "\n",
    "            if(numpy_arr1.size == numpy_arr2.size):\n",
    "                \n",
    "                \n",
    "                corr, _ = pearsonr(numpy_arr1, numpy_arr2)\n",
    "                \n",
    "                #this threshold of 0.8 can be changed according to use case \n",
    "                if(corr>0.8):\n",
    "                    print('Pearsons correlation between {} and {} : {}'.format(p[0], p[1], corr))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Pearson Correlation: {}'.format(corr)])\n",
    "\n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                \n",
    "                #this threshold of 0.3 can be changed according to use case \n",
    "                if(jacc_score>0.3):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and there have equal number of values\".format(p[0], p[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    r1+=1\n",
    "\n",
    "\n",
    "                d1 = fit_distribution(df1, p[0],0.99,0.01)\n",
    "                d2 = fit_distribution(df2, p[1],0.99,0.01)\n",
    "\n",
    "                m1 = mean(numpy_arr1)\n",
    "                m2 = mean(numpy_arr2)\n",
    "\n",
    "                st1 = std(numpy_arr1)\n",
    "                st2 = std(numpy_arr2)\n",
    "\n",
    "                if(max(m1,m2)!=0):\n",
    "                    m = (abs(m1-m2)/max(m1,m2))*100\n",
    "\n",
    "                if(max(st1,st2)!=0):\n",
    "                    s = (abs(st1-st2)/max(st1,st2))*100\n",
    "\n",
    "                try:\n",
    "                    #checking if the difference in mean is less than 5% of the max mean and similarly for std.\n",
    "                    if(m<5 and s<5 and d1==d2):\n",
    "                        print(\"Both {} and {} have mean and standard deviation within 5% \".format(p[0], p[1]))\n",
    "                        attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                        attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                        metric.append(['Mean and Std. are within 5% and both follow {} distribution'.format(d1)])\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                #append the list for graph representation\n",
    "                if(d1==d2 and jacc_score>0.3):\n",
    "\n",
    "                    str1=get_df_name(df1)+'__'+p[0]\n",
    "                    str2=get_df_name(df2)+'__'+p[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "                    \n",
    "                if(corr>0.8 and d1==d2):\n",
    "                    s1+=1\n",
    "                        \n",
    "                        \n",
    "            #when length of attributes are different, here we wont calculate correlation\n",
    "            else:\n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                if(jacc_score>0.5):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and there have equal number of values\".format(p[0], p[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    \n",
    "                    r1+=1\n",
    "\n",
    "\n",
    "                d1 = fit_distribution(df1, p[0],0.99,0.01)\n",
    "                d2 = fit_distribution(df2, p[1],0.99,0.01)\n",
    "\n",
    "                m1 = mean(numpy_arr1)\n",
    "                m2 = mean(numpy_arr2)\n",
    "\n",
    "                st1 = std(numpy_arr1)\n",
    "                st2 = std(numpy_arr2)\n",
    "\n",
    "                if(max(m1,m2)!=0):\n",
    "                    m = (abs(m1-m2)/max(m1,m2))*100\n",
    "\n",
    "                if(max(st1,st2)!=0):\n",
    "                    s = (abs(st1-st2)/max(st1,st2))*100\n",
    "\n",
    "                try:\n",
    "                    if(m<5 and s<5 and d1==d2):\n",
    "                        print(\"Both {} and {} have mean and standard deviation within 5% \".format(p[0], p[1]))\n",
    "                        attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                        attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                        metric.append(['Mean and Std. are within 5% and both follow {} distribution'.format(d1)])\n",
    "                        \n",
    "                        s1+=1\n",
    "\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                if(d1==d2 and jacc_score>0.5):\n",
    "\n",
    "                    str1=get_df_name(df1)+'__'+p[0]\n",
    "                    str2=get_df_name(df2)+'__'+p[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "    return r1,s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Comparator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all numerical attributes b/w the 2 datasets and finds similarity and replication using Jaccard similarity, correlation, distribution test and other statistics\n",
    "\n",
    "def num_compare(df1, df2, list_num):\n",
    "    \n",
    "    #initialising overall similarity and replication counter\n",
    "    s2=0\n",
    "    r2=0\n",
    "\n",
    "    for p in list_num:\n",
    "        x = re.search('_num_days$', p[0])\n",
    "        y = re.search('_num_days$', p[1])\n",
    "\n",
    "        #only if both columns not ends with _num_days (numerical column) then will be processed\n",
    "        if(x==None and y==None):\n",
    "\n",
    "\n",
    "            len1 = len(df1[p[0]])\n",
    "            len2 = len(df2[p[1]])\n",
    "\n",
    "            len1uniq=df1[p[0]].nunique(dropna=False) #length of unique values\n",
    "            len2uniq=df2[p[1]].nunique(dropna=False)\n",
    "\n",
    "            numpy_df1_uniq  = df1[p[0]].unique()\n",
    "            numpy_df2_uniq  = df2[p[1]].unique()\n",
    "            \n",
    "            \n",
    "            percent_unique = (abs(len1uniq-len2uniq)/max(len1uniq,len2uniq))*100\n",
    "\n",
    "            numpy_df1 = df1[p[0]].dropna()\n",
    "            numpy_df2 = df2[p[1]].dropna()\n",
    "\n",
    "            numpy_arr1=numpy_df1.to_numpy()\n",
    "            numpy_arr2=numpy_df2.to_numpy()\n",
    "            \n",
    "            #when length of both columns are equal\n",
    "\n",
    "            if(numpy_arr1.size == numpy_arr2.size):\n",
    "                \n",
    "                #correlation will only be calculated when lengths are equal\n",
    "                corr, _ = pearsonr(numpy_arr1, numpy_arr2)\n",
    "                \n",
    "                \n",
    "                #this threshold can be changed\n",
    "                if(corr>0.8):\n",
    "                    print('Pearsons correlation between {} and {} : {}'.format(p[0], p[1], corr))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Pearson Correlation: {}'.format(corr)])\n",
    "\n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                \n",
    "                #this 0.3 can be changed\n",
    "                if(jacc_score>0.3):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and there have equal number of values\".format(p[0], p[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    r2+=1\n",
    "\n",
    "\n",
    "                d1 = fit_distribution(df1, p[0],0.99,0.01)\n",
    "                d2 = fit_distribution(df2, p[1],0.99,0.01)\n",
    "\n",
    "                m1 = mean(numpy_arr1)\n",
    "                m2 = mean(numpy_arr2)\n",
    "\n",
    "                st1 = std(numpy_arr1)\n",
    "                st2 = std(numpy_arr2)\n",
    "\n",
    "                if(max(m1,m2)!=0):\n",
    "                    m = (abs(m1-m2)/max(m1,m2))*100\n",
    "\n",
    "                if(max(st1,st2)!=0):\n",
    "                    s = (abs(st1-st2)/max(st1,st2))*100\n",
    "\n",
    "                try:\n",
    "                    if(m<5 and s<5 and d1==d2):\n",
    "                        print(\"Both {} and {} have mean and standard deviation within 5% \".format(p[0], p[1])) \n",
    "                        attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                        attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                        metric.append(['Mean and Std. are within 5% and both follow {} distribution'.format(d1)])\n",
    "                        \n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                if(d1==d2 and jacc_score>0.5):\n",
    "\n",
    "                    str1=get_df_name(df1)+'__'+p[0]\n",
    "                    str2=get_df_name(df2)+'__'+p[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "                    \n",
    "                if(corr>0.8 and d1==d2):\n",
    "                    s2+=1\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "            elif(len1!=len2 and len1uniq==len2uniq):\n",
    "                              \n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                if(jacc_score>0.5):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and there have equal number of values\".format(p[0], p[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                              \n",
    "                    r2+=1\n",
    "\n",
    "                d1 = fit_distribution(df1, p[0],0.99,0.01)\n",
    "                d2 = fit_distribution(df2, p[1],0.99,0.01)\n",
    "                    \n",
    "\n",
    "                m1 = mean(numpy_arr1)\n",
    "                m2 = mean(numpy_arr2)\n",
    "\n",
    "                st1 = std(numpy_arr1)\n",
    "                st2 = std(numpy_arr2)\n",
    "\n",
    "                if(max(m1,m2)!=0):\n",
    "                    m = (abs(m1-m2)/max(m1,m2))*100\n",
    "\n",
    "                if(max(st1,st2)!=0):\n",
    "                    s = (abs(st1-st2)/max(st1,st2))*100\n",
    "\n",
    "                try:\n",
    "                    if(m<5 and s<5):\n",
    "                        print(\"Both {} and {} have mean and standard deviation within 5% \".format(p[0], p[1]))\n",
    "                        attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                        attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                        metric.append(['Mean and Std. are within 5% and both follow {} distribution'.format(d1)])\n",
    "                        s2+=1\n",
    "\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                if(d1==d2 and jacc_score>0.5):\n",
    "\n",
    "                    str1=get_df_name(df1)+'__'+p[0]\n",
    "                    str2=get_df_name(df2)+'__'+p[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "                    \n",
    "                \n",
    "                        \n",
    "            #this condition ensure that the %unique b/w two columns is not very high            \n",
    "            elif(percent_unique<90):\n",
    "                              \n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                if(jacc_score>0.5):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and there have equal number of values\".format(p[0], p[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                              \n",
    "                    r2+=1\n",
    "\n",
    "                d1 = fit_distribution(df1, p[0],0.99,0.01)\n",
    "                d2 = fit_distribution(df2, p[1],0.99,0.01)\n",
    "                    \n",
    "\n",
    "                m1 = mean(numpy_arr1)\n",
    "                m2 = mean(numpy_arr2)\n",
    "\n",
    "                st1 = std(numpy_arr1)\n",
    "                st2 = std(numpy_arr2)\n",
    "\n",
    "                if(max(m1,m2)!=0):\n",
    "                    m = (abs(m1-m2)/max(m1,m2))*100\n",
    "\n",
    "                if(max(st1,st2)!=0):\n",
    "                    s = (abs(st1-st2)/max(st1,st2))*100\n",
    "\n",
    "                try:\n",
    "                    if(m<5 and s<5):\n",
    "                        print(\"Both {} and {} have mean and standard deviation within 5% \".format(p[0], p[1]))\n",
    "                        attribute1.append(get_df_name(df1)+'__'+p[0])\n",
    "                        attribute2.append(get_df_name(df2)+'__'+p[1])\n",
    "                        metric.append(['Mean and Std. are within 5% and both follow {} distribution'.format(d1)])\n",
    "                        s2+=1\n",
    "\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                if(d1==d2 and jacc_score>0.5):\n",
    "\n",
    "                    str1=get_df_name(df1)+'__'+p[0]\n",
    "                    str2=get_df_name(df2)+'__'+p[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "                    \n",
    "                    \n",
    "    return r2, s2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Comparator Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all object attributes b/w the 2 datasets and finds similarity and replication using Jaccard similarity, fuzzy algorithm (edit distance) and other statistics\n",
    "def txt_compare(df1, df2, list_txt):\n",
    "    \n",
    "    \n",
    "    #initialising overall similarity and replication counter\n",
    "    r3=0\n",
    "    s3=0\n",
    "\n",
    "    for t in list_txt:\n",
    "            len1 = len(df1[t[0]])\n",
    "            len2 = len(df2[t[1]])\n",
    "\n",
    "            len1uniq=df1[t[0]].nunique(dropna=False)\n",
    "            len2uniq=df2[t[1]].nunique(dropna=False)\n",
    "\n",
    "            numpy_df1_uniq  = df1[t[0]].unique()\n",
    "            numpy_df2_uniq  = df2[t[1]].unique()\n",
    "            \n",
    "\n",
    "            numpy_arr1_uniq = numpy_df1_uniq.tolist()\n",
    "            numpy_arr2_uniq = np.sort(numpy_df2_uniq).tolist()\n",
    "\n",
    "\n",
    "            numpy_df1 = df1[t[0]].to_numpy()\n",
    "            numpy_df2 = df2[t[1]].to_numpy()\n",
    "\n",
    "\n",
    "            numpy_arr1=numpy_df1.tolist()\n",
    "            numpy_arr2=numpy_df2.tolist()\n",
    "            \n",
    "            percent_unique = (abs(len1uniq-len2uniq)/max(len1uniq,len2uniq))*100\n",
    "\n",
    "            #same length but different unique length\n",
    "            if(len1 == len2 and len1uniq!=len2uniq):\n",
    "\n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                if(jacc_score>0.5):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and they are of same length\".format(t[0], t[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    r3+=1\n",
    "\n",
    "                fuzz_ratio = fuzz.partial_ratio(numpy_arr1,numpy_arr2)\n",
    "                if(fuzz_ratio>30):\n",
    "                    print(\"There is significant Fuzzy ratio b/w {} and {} : {} and they are of same length\".format(t[0], t[1], fuzz_ratio))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant fuzzy ratio: {}'.format(fuzz_ratio)])\n",
    "                    s3+=1\n",
    "                    \n",
    "                if(jacc_score>0.5 or fuzz_ratio>40):\n",
    "                    str1=get_df_name(df1)+'__'+t[0]\n",
    "                    str2=get_df_name(df2)+'__'+t[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "                    \n",
    "            elif(len1 == len2 and len1uniq==len2uniq):\n",
    "\n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                if(jacc_score>0.3):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {} and they are of same length and consist of equal number of unique value\".format(t[0], t[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    r3+=1\n",
    "\n",
    "                fuzz_ratio = fuzz.partial_ratio(numpy_arr1,numpy_arr2)\n",
    "                if(fuzz_ratio>30):\n",
    "                    print(\"There is significant Fuzzy ratio b/w {} and {} : {} and they are of same length and consist of equal number unique value\".format(t[0], t[1], fuzz_ratio))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant fuzzy ratio: {}'.format(fuzz_ratio)])\n",
    "                    s3+=1\n",
    "                    \n",
    "                if(jacc_score>0.3 and fuzz_ratio>40):\n",
    "                    str1=get_df_name(df1)+'__'+t[0]\n",
    "                    str2=get_df_name(df2)+'__'+t[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "\n",
    "\n",
    "            #no of unique are same\n",
    "            elif(len1 != len2 and len1uniq==len2uniq):\n",
    "\n",
    "                jacc_score = jaccard_similarity(numpy_arr1_uniq,numpy_arr2_uniq)\n",
    "                if(jacc_score>0.3):\n",
    "                    print(\"There is significant jaccard similarity b/w unique value of {} and {} : {}\".format(t[0], t[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    r3+=1\n",
    "\n",
    "                fuzz_ratio = fuzz.partial_ratio(numpy_arr1_uniq,numpy_arr2_uniq)\n",
    "                if(fuzz_ratio>30):\n",
    "                    print(\"There is significant Fuzzy ratio b/w {} and {} : {} and they are of same number of unique value\".format(t[0], t[1], fuzz_ratio))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant fuzzy ratio: {}'.format(fuzz_ratio)])\n",
    "                    s3+=1\n",
    "                    \n",
    "                if(jacc_score>0.3 and fuzz_ratio>40):\n",
    "                    str1=get_df_name(df1)+'__'+t[0]\n",
    "                    str2=get_df_name(df2)+'__'+t[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "\n",
    "            #this condition ensure that the %unique b/w two columns is not very high \n",
    "            elif(percent_unique<90):\n",
    "                jacc_score = jaccard_similarity(numpy_arr1,numpy_arr2)\n",
    "                if(jacc_score>0.3):\n",
    "                    print(\"There is significant jaccard similarity b/w {} and {} : {}\".format(t[0], t[1], jacc_score))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant Jaccard Similarity: {}'.format(jacc_score)])\n",
    "                    r3+=1\n",
    "                fuzz_ratio = fuzz.partial_ratio(numpy_arr1,numpy_arr2)\n",
    "                if(fuzz_ratio>30):\n",
    "                    print(\"There is significant Fuzzy ratio b/w {} and {} : {} \".format(t[0], t[1], fuzz_ratio))\n",
    "                    attribute1.append(get_df_name(df1)+'__'+t[0])\n",
    "                    attribute2.append(get_df_name(df2)+'__'+t[1])\n",
    "                    metric.append(['Significant fuzzy ratio: {}'.format(fuzz_ratio)])\n",
    "                    s3+=1\n",
    "                    \n",
    "                if(jacc_score>0.3 and fuzz_ratio>40):\n",
    "                    str1=get_df_name(df1)+'__'+t[0]\n",
    "                    str2=get_df_name(df2)+'__'+t[1]\n",
    "\n",
    "                    list_lineage_source.append(str1)\n",
    "                    list_lineage_target.append(str2)\n",
    "\n",
    "    return r3, s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that create lineage between 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take two datasets as argument and will do the following:\n",
    "# 1) Make 2 list : a) list of combination of all numerical datatype and b) list of combination of text datatype\n",
    "# 2) Then three functions will be called where all replication and similarity test will be carried for all combinations of attributes\n",
    "# 3) Also the overall degree of similarity and replication will be printed in the end\n",
    "\n",
    "def compare_two_tables(df1, df2):\n",
    "    \n",
    "    n1 = len(df1.columns)\n",
    "    n2 = len(df2.columns)\n",
    "    \n",
    "    total_columns = n1+n2\n",
    "    \n",
    "    #df1\n",
    "    filteredColumns_txt1 = df1.dtypes[df1.dtypes == np.object ]\n",
    "    # list of columns whose data type is object string\n",
    "    listOfColumnNames_txt1 = list(filteredColumns_txt1.index)\n",
    "\n",
    "    filteredColumns_num1 = df1.dtypes[df1.dtypes == np.int64 ] + df1.dtypes[df1.dtypes == np.float64 ]\n",
    "    # list of columns whose data type is object int or float\n",
    "    listOfColumnNames_num1 = list(filteredColumns_num1.index)\n",
    "    \n",
    "    \n",
    "    #df2\n",
    "    filteredColumns_txt2 = df2.dtypes[df2.dtypes == np.object ]\n",
    "    listOfColumnNames_txt2 = list(filteredColumns_txt2.index)\n",
    "\n",
    "    filteredColumns_num2 = df2.dtypes[df2.dtypes == np.int64 ] + df2.dtypes[df2.dtypes == np.float64 ]\n",
    "    listOfColumnNames_num2 = list(filteredColumns_num2.index)\n",
    "    \n",
    "    \n",
    "    #combination of every couple of columns\n",
    "    list_num = list(itertools.product(listOfColumnNames_num1, listOfColumnNames_num2))\n",
    "    list_txt = list(itertools.product(listOfColumnNames_txt1, listOfColumnNames_txt2))\n",
    "    \n",
    "    \n",
    "    #r1, r2, r3 and s1,s2,s3 will be returned from the functions which then will be used to find overall replication and similarity\n",
    "    r1, s1 = dates_compare(df1, df2, list_num)\n",
    "    r2, s2 = num_compare(df1, df2, list_num)\n",
    "    r3, s3 = txt_compare(df1, df2, list_txt)\n",
    "    \n",
    "    #overall degree of replication between df1 and df2\n",
    "\n",
    "    overall_degree_replication = (r1+r2+r3)/max(n1,n2)\n",
    "    \n",
    "    #overall degree of similarity between df1 and df2\n",
    "\n",
    "    overall_degree_similarity = (s1+s2+s3)/max(n1,n2)\n",
    "        \n",
    "    return overall_degree_replication, overall_degree_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list of combination of all datasets between two databases\n",
    "list_table_combination=[]\n",
    "for i in range(len(list_table1)):\n",
    "    for j in range(len(list_table2)):\n",
    "        list_table_combination.append([globals()['{}__{}'.format(database_name1, list_table1[i]) ], globals()['{}__{}'.format(database_name2, list_table2[j]) ]])\n",
    "        \n",
    "\n",
    "\n",
    "#all combination of datasets b/w the 2 database will pass thorugh the compare_two_tables (MOST TIME CONSUMING PART)\n",
    "for t in list_table_combination:\n",
    "    a, b = compare_two_tables(t[0], t[1])\n",
    "    print(\"Overall degree of replication between {} and {} is {}\".format(get_df_name(t[0]), get_df_name(t[1]), a))\n",
    "    print(\"Overall degree of similarity between {} and {} is {}\".format(get_df_name(t[0]), get_df_name(t[1]), b))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT 1: Tabular Data Lineage report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df will contain the all the detailed imformation of the lineage\n",
    "main_df = pd.DataFrame(list(zip(attribute1, attribute2, metric)),columns=['Database 1','Database 2','Metric'])\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT 2 A: Graphical Data Lineage report using networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list which were initialised earlier were filled when the compare_tables function was called and these lost will be used to \n",
    "# form a lineage graph\n",
    "\n",
    "#This lineage graph contains only highly replicated attributes\n",
    "\n",
    "# set size\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.axis('off')\n",
    "\n",
    "lineage_df = pd.DataFrame(list(zip(list_lineage_source, list_lineage_target)),columns=['Source','Target'])\n",
    "FG = nx.from_pandas_edgelist(lineage_df, source='Source', target='Target', edge_attr=None,)\n",
    "\n",
    "\n",
    "nx.draw_networkx(FG, with_labels = True)\n",
    "\n",
    "l,r = plt.xlim()\n",
    "plt.xlim(l-2,r+2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT 2 B: Graphical Data Lineage report using Pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lineage graph contains both highly replicated and highly similar attributes\n",
    "\n",
    "l1=main_df['Database 1'].tolist()\n",
    "l2 = main_df['Database 2'].tolist()\n",
    "from pyvis.network import Network\n",
    "lineage_df = pd.DataFrame(list(zip(l1, l2)),columns=['Source','Target'])\n",
    "FG = nx.from_pandas_edgelist(lineage_df, source='Source', target='Target', edge_attr=None)\n",
    "\n",
    "net = Network(notebook=True,height='1000px',width='1000px', heading='Lineage')\n",
    "net.from_nx(FG, default_edge_weight=15, default_node_size=25)\n",
    "\n",
    "net.show('list_of_nodes.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras: Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will display a picture of full schema graph of both relational databases\n",
    "import pydot\n",
    "import sqlalchemy_schemadisplay\n",
    "import graphviz\n",
    "from sqlalchemy_schemadisplay import create_schema_graph\n",
    "from sqlalchemy import MetaData\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "graph1 = create_schema_graph(metadata=MetaData('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    user_name1, \n",
    "    password1, \n",
    "    host1, \n",
    "    port1, \n",
    "    database_name1 )))\n",
    "\n",
    "graph2.write_png('dbschema1.png')\n",
    "\n",
    "Image(filename='dbschema1.png') \n",
    "\n",
    "\n",
    "graph2 = create_schema_graph(metadata=MetaData('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    user_name2, \n",
    "    password2, \n",
    "    host2, \n",
    "    port2, \n",
    "    database_name2 )))\n",
    "\n",
    "graph2.write_png('dbschema2.png')\n",
    "\n",
    "Image(filename='dbschema2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting metadata from database and storing it in pandas dataframe\n",
    "\n",
    "query_keys = '''\n",
    "                select \n",
    "                tco.constraint_name,\n",
    "                tco.constraint_type,\n",
    "                kcu.table_schema,\n",
    "                kcu.table_name,\n",
    "                ccu.table_schema,\n",
    "                ccu.table_name,\n",
    "                string_agg(distinct ccu.column_name, '; ')\n",
    "            from information_schema.table_constraints tco\n",
    "            join information_schema.key_column_usage kcu \n",
    "                 on kcu.constraint_name = tco.constraint_name\n",
    "                 and kcu.constraint_schema = tco.constraint_schema\n",
    "                 and kcu.constraint_name = tco.constraint_name\n",
    "            JOIN information_schema.constraint_column_usage ccu ON ccu.constraint_name = tco.constraint_name\n",
    "            where \n",
    "                ( tco.constraint_type = 'PRIMARY KEY' or  tco.constraint_type = 'FOREIGN KEY' )\n",
    "                and tco.constraint_schema = 'public'\n",
    "            group by tco.constraint_name,\n",
    "                tco.constraint_type,\n",
    "                kcu.table_schema,\n",
    "                kcu.table_name,\n",
    "                ccu.table_schema,\n",
    "                ccu.table_name;\n",
    "                \n",
    "            '''\n",
    "\n",
    "df_keys = pd.read_sql_query(query_keys, engine)\n",
    "df_keys = pd.DataFrame(df_keys)\n",
    "df_keys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
